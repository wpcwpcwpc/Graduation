Reviewer 8 of SMC 2021 submission 250

Comments to the author
======================

It is an interesting research. The subject is relevant. The
novelty is clear. The writing is OK. Although, I have a few
comments about the text and the methodology.

Abstract: Clarify the abbreviations. ¡°A-PSPNet¡±, which is
in the title, is not fully understandable in the abstract,
as ¡°attention-based¡± is not capitalized and neither the
abbreviation is followed inside parentheses. So far, ¡°CBAM¡±
is also unknown. I believe MIoU and MPQ is not worth being
written in the abstract.

Introduction: I do not understand what the authors mean
with the phrase: ¡°FCN emerges as the times require¡±. Also,
I believe the bold word ¡°Contribution¡± should be removed.

Related work: There is a missing citation in ¡°...training
cost. Hariharan et al. present a hyper-column...¡±.

Fig.1: It is hard to see the whole picture in this figure.
There is no input or output. The bottleneck is the unit of
a ResNet which is inside a CBAM? How is each pyramid pool
different from each other? After the pyramid parsing, what
happens?

Fig.2: Specify the information of each dimension of the
input and output.

Eq.1: I do not fully understand it. Some variables are not
defined.

Fig.3: Isn¡¯t the MLP in the wrong direction? 

Eq.3: Does the ¡°plus symbol¡± mean sum? Fig. 3 indicates an
operation of concatenation.

III-B. A-PSPNet: how the upsampling is computed?

III-C. Combo Loss: Interesting function, a reference should
be included for readers that would like more details about
it.

Eq.6: Is the equation complete? Isn¡¯t it missing some
parameters of the sum symbols?

IV-A. Dataset: the database is very interesting and I
believe the authors should make it publicly available, if
possible. As I understood, the database holds several
images from the same 17 patients, most of them extracted
from frames of videos, so it might have images very similar
and, if the sets were randomly divided, they might have
very similar images from the same video of the same kidney
into both training and validation sets, making the
segmentation easier. Also, the experiments were conducted
with just one holdout. Considering a private database,
possibility of similar images of the same kidney in
training and validation set and, finally, just one holdout,
I believe the results are slightly questionables. The
experiments should be executed with cross-validation or
several random holdouts. Also, the same kidneys should be
either the training or the validation set, never in both of
them. More explanations about the definition of the
experimental protocol should be included in the paper, to
answer questions like: 
-> Why is the database not publicly available on some
repository such as Figshare?
-> Why is it only one holdout? Why not several holdouts or
cross-validation? Is it a computational time issue? Is it a
common practice in (medical) image segmentation?
-> Why use the rate 9:1 and how was it divided? Randomly?

III-B. Implementation details: Fig. 4 indicates the images
have different sizes. How is the input resolution fixed?
The initial weights were generated by a general purpose
database? 

IV-C. Performance Indicators: it would be good to have
references of other segmentation researches that use the
same indicators.

Eqs.7,8: are they corrected? There is a parameter
missing in a sigma. ¡°k+1 is the number of classes¡±, what
classes?

Results: check the capitalization of the name of the
subsections. The phrase ¡°Relatively speaking, the attention
model that goes through the channel domain first and then
the spatial domain has the best effect.¡± demonstrate a
quite simple idea, but it is really hard to understand.

Table2: it does not seem fair to compare one attention
unit with the whole ResNet50.

Table3: it is in the middle of a paragraph. It would be
good to have the reference of each model cited in tables 2
and 3.

V-C. Comparison of effectiveness: Is the PSPNet from table
3 somehow related to the ResNet50 from table 2? If they
are, the gain in the indicators are almost the same as the
overhead. If they are related, it should be discussed. If
they are not related, it should be clarified. Why are the
networks from table 3 not discussed in table 2? If the
lightness is so important, at least the models that present
the top 2 indicators in table 3 should be compared in table
2. Also, I believe it should be 1.32 percentage point (pp),
not ¡°a 1.32% improvement¡±.

VI-Conclusion:
-> the overhead can not be ignored, but it can be compared
to the gain in the results.
-> ¡°...experiments on data sets obtained...¡± I believe they
were obtained with one database.
-> ¡°...Our experiment shows that compared with the model
before adding the attention module, the calculation amount
and the number of parameters of the model increase less and
the performance improves, which proves the lightness and
effectiveness of the model.¡± I believe it should be ¡°Our
experiments show¡±. The parameters of the model Increase
less than what?
-> I believe the results ¡°indicate¡± or ¡±demonstrate¡± the
effectiveness, due to the issues of the experimental
methodology and the database, also due to the fact that the
experiments were conducted in only one non-openly database
and without statistical tests comparing results from other
well-known models.

References: they are not completely in the ieeetrans
bibliography style.